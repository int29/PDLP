{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafd0166-f68a-4c31-8f1a-1f444f9c373b",
   "metadata": {},
   "source": [
    "# Practical Deep-Learning using Pytorch\n",
    "* project start : 2023-03-23\n",
    "* project end : \n",
    "* wirter : Int29\n",
    "* github : https://github.com/int29/PDLP\n",
    "* project description : 처음부터 다시 딥러닝 공부를 하면서 공부한 내용을 정리함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c4f06-7750-4541-a393-b192afdd0340",
   "metadata": {},
   "source": [
    "## Chapter01 : classical artificial neural network(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f247d-a7d4-4686-9045-6d50391a742a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 01.XOR논리 게이트로 살펴보는 퍼셉트론(Perceptron)의 한계\n",
    "\n",
    "AND, OR, NAND 게이트와 마찬가지로 [그림2-1] XOR문제를 퍼셉트론  통해서 해결해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64b52b",
   "metadata": {},
   "source": [
    "퍼셉트론은 MCP뉴런의 작동원리에 기반하여 입력신호에 편향(bias)를 추가해 활성화함수에 전달한다. 활성화 함수는 신호를 0과1의 출력값으로 변환하여 출력하는데, 이 때 퍼셉트론의 학습알고리즘은 각 신호에 곱하는 최적 가중치($w$)와 더하는 편향($b$)을 주어진 데이터를 잘 분류할 수 있도록 조절하여 학습하게 된다. 즉 퍼셉트론의 파라미터는 가중치($w$)와 더하는 편향($b$)이며 퍼셉트론은 분류정확도를 가장 높일 수 있는 최적 파라미터를 찾는다. \n",
    "<br><br><br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/int29/PDLP/blob/main/ch01_classical_neural_network/img/01_02.png?raw=true\" width=\"900\">\n",
    "    <span>[그림01-02]</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220488b",
   "metadata": {},
   "source": [
    "### (1-1) 퍼셉트론의 파라미터 가중치와 편향 \n",
    "퍼셉트론은 복수의 입력에 개별 가중치를 곱하고 편향을 더한다. 때문에 가중치($w$)에 따라 특정 입력이 활성화에 강하게 영향을 미칠수도 반대로 적게 미치게될 수도 있다. 또한 편향($b$)을 더해주기 때문에 이 편향값에 따라 약한 신호에서도 활성화가 잘될지(민감) 아니면 강한신호에도 활성화가 잘 되지 않을지(둔감)하게 될 수도 있다.\n",
    "\n",
    "- 가중치($w$) : 여러 입력 신호 중 출력신호에 영향을 주는 중요도를 결정한다.\n",
    "- 편향($b$) : 활성함수가 입력신호에 대해 얼마나 민감하게 반응할지 결정한다.\n",
    "\n",
    "입력신호는 $x=[1,1,1]$와 같고 각 입력신호에 전달되는 가중치가 $w=[0.1,0.2,10]$이라면 활성함수에 전달되는 값은 $1\\times0.1 + 1\\times0.2 + 1\\times10$이 될것이다. 따라서 3번째 신호의 가중치가 10이기 때문에 활성화여부에 가장 많은 영향을 주는것을 직관적으로 알 수 있다. \n",
    "\n",
    "이 때 만약 임계치가 $\\theta=15$라면 위 입력신호의 가중합은 10.3이기 때문에 활성화 되지 않을것이다. 하지만 임계치가 같을 때 편향($b$) $=5$ 라면 입력신호의 가중합 10.3에 편향 5가 더해져 15.5가 활성함수에 입력돼 활성화된 값인 1을 출력한다. 극단적으로 편향($b$)$=15$ 라면 입력신호나 가중치에 관계없이 항상 임계치보다 높기 때문에 무조건 1을 출력하게될것이다. 즉 편향(bias)은 뉴런이 입력신호에 얼마나 민감하게 반응할지 결정한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976794e",
   "metadata": {},
   "source": [
    "### (1-2) 활성화 함수(activation function)\n",
    "활성화 함수(activation function)는 인공신경망에서 연속값인 입력 신호의 총합을 0과1의 이진분류 값으로 변환하는 함수이다. 다양한 활성화함수가 존재하며 Task에 맞는 활성함수가 없다면 직접 설계하여 만들 수도 있다.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/int29/PDLP/blob/main/ch01_classical_neural_network/img/01_04.png?raw=true\" width=\"400\">\n",
    "    <span>[그림01-03]</span>\n",
    "</div>\n",
    "\n",
    "가장 기본적인 활성함수는 계단함수(Step Function)이다. 계단함수는 간단해 이해하기 쉽기 때문에 계단함수를 통해서 퍼셉트론의 활성화 함수 작동에 대해 살펴보자. \n",
    "\n",
    "계단함수는 입력되는 값이 임계값($\\theta$)보다 높으면 1을 출력하고, 낮으면 0을 출력한다. 위 예시에서 임계값($\\theta$)이 15였기 때문에 계단함수는 아래와 같이 나타낼 수 있다.\n",
    "\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/int29/PDLP/blob/main/ch01_classical_neural_network/img/01_05.png?raw=true\" width=\"400\">\n",
    "    <span>[그림01-04]</span>\n",
    "</div>\n",
    "\n",
    "$x$ 축은 활성화함수에 최종적으로 전달되는 입력값이며 $y$ 축은 입력값에 따른 활성함수의 출력값을 나타낸다. 활성함수인 계단함수를 살펴보면 입력되는 신호가 임계치인 15가 넘기전까지 $y$ 값은 모두 0으로 출력되고 입력값이 15이상부터는 $y$가 1로 출력되는것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205be86",
   "metadata": {},
   "source": [
    "### (1-3) 퍼셉트론의 학습알고리즘\n",
    "\n",
    "퍼셉트론은 지도학습 알고리즘의 한 종류로 입력데이터에 대한 예측값과 실제값의 차이를 적게 만들도록 가중치($w$)와 더하는 편향($b$)을 조절하며 아래와 같은 방식으로 최적 가중치와 편향을 학습한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01beb4",
   "metadata": {},
   "source": [
    "가중치와 편향을 결정하기 위해 각각의 훈련 샘플 $x^{(i)}$에 대하여 다음의 단계를 수행한다. $^{[2]}$\n",
    "\n",
    "STEP01 : 가중치와 편향을 0 혹은 임의의 작은 숫자로 초기화한다.<br>\n",
    "STEP02 : 예측값 $\\hat{y}$ 와 실제값 $y$ 를 비교한 결과를 토대로 가중치($w$)와 편향($b$)을 업데이트한다. \n",
    "\n",
    "* (가중치) $w_{j} :=w_{j}+\\Delta w_{j}$ 이 때, $\\Delta w_{j} = \\eta(y^{(i)}-\\hat{y^{(i)}})x^{i}$펴\n",
    "* (편향)$b :=b+\\Delta b$  이 때 $\\Delta b = \\eta(y^{(i)}-\\hat{y^{(i)}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f7a0c",
   "metadata": {},
   "source": [
    "훈련샘플($x$)과 실제값($y$)으로 구성된 훈련데이터가 아래와 같을때, 실제 퍼셉트론이 학습하는 알고리즘을 살펴보자.\n",
    "\n",
    "$x = \\{1,2,3\\}$, $y=\\{0,0,1\\}$ ,  $\\hat{y}=\\{1,1,1\\}$ , $\\eta = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759bfc40",
   "metadata": {},
   "source": [
    "**STEP01 : 가중치와 편향을 0 혹은 임의의 작은 숫자로 초기화한다.**\n",
    "\n",
    "- $w_{0}$=0 , $b_{0}$=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6bc70",
   "metadata": {},
   "source": [
    "**STEP02 : 예측값 $\\hat{y}$ 와 실제값 $y$ 를 비교한 결과를 토대로 가중치($w$)와 편향($b$)을 업데이트한다.** \n",
    "\n",
    "**<<첫번째 결과>**\n",
    "\n",
    "- $\\Delta w_{1} = \\eta(y^{(1)}-\\hat{y^{(1)}})x^{1} = 0.5\\cdot(0 - 1)\\cdot1 = -0.5$<br><br>\n",
    "- $w_{1} :=w_{0}+\\Delta w_{1}=0 + (-0.5) = -0.5$<br><br>\n",
    "\n",
    "- $\\Delta b = \\eta(y^{(1)}-\\hat{y^{(1)}})=0.5\\cdot(0-1)=-0.5$<br><br>\n",
    "- $b_{1} :=b_{0}+\\Delta b_{1}=0+(-0.5)=-0.5$\n",
    "\n",
    "첫번째 예측값은 0이고 실제값은 1로 퍼셉트론은 실제와 다른 결과를 예측하였다. 따라서 주어진 데이터에 맞게 예측할 수 있도록 가중치와 편향을 조정해야한다. 두 값의 차이가 -1이기 때문에 학습률($\\eta$) 0.5를 곱한 -0.5만큼 가중치와 편향을 업데이트 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218e379",
   "metadata": {},
   "source": [
    "**<두번째 결과>**\n",
    "\n",
    "- $\\Delta w_{2} = \\eta(y^{(2)}-\\hat{y^{(2)}})x^{2} = 0.5\\cdot(0 - 1)\\cdot2 = -1$<br><br>\n",
    "- $w_{1} :=w_{1}+\\Delta w_{2}=-0.5 + (-1) = -1.5$<br><br>\n",
    "\n",
    "- $\\Delta b = \\eta(y^{(2)}-\\hat{y^{(2)}})=0.5\\cdot(0-1)=-0.5$<br><br>\n",
    "- $b_{2} :=b_{1}+\\Delta b_{2}=-0.5+(-0.5)=-1$\n",
    "\n",
    "두번째 결과도 첫번째와 마찬가지로 예측값은 0이지만 실제값은 1이기 때문에 마이너스(-) 방향으로 가중치를 한번 더 업데이트 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb7e7c",
   "metadata": {},
   "source": [
    "**<세번째 결과>**\n",
    "\n",
    "- $\\Delta w_{3} = \\eta(y^{(3)}-\\hat{y^{(3)}})x^{3} = 0.5\\cdot(1 - 1)\\cdot3 = 0$<br><br>\n",
    "- $w_{3} :=w_{2}+\\Delta w_{3}=-1.5 + 0 = -1.5$<br><br>\n",
    "\n",
    "- $\\Delta b = \\eta(y^{(3)}-\\hat{y^{(3)}})=0.5\\cdot(1-1)= 0$<br><br>\n",
    "- $b_{3} :=b_{2}+\\Delta b_{3}=-1+0=-1$\n",
    "\n",
    "세번째 결과에서는 예측값과 실제값이 동일했기 때문에 학습알고리즘은 더이상 기존 훈련하여 결정하였던 가중치와 편향을 업데이트하지 않는다. \n",
    "\n",
    "모든 데이터에 대해서 예측결과와 실제값을 비교하였기 때문에 최종적으로 퍼셉트론의 학습알고리즘은 데이터를 기반으로 가중치($w$) 는 -1.5로 편향($b$)는 -1로 학습을 완료한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bcd093",
   "metadata": {},
   "source": [
    "### 퍼셉트론의 하이퍼파라미터\n",
    "퍼셉트론은 가중치와 편향외에도 사용자가 학습을 위해 조절해줘야하는 값이 존재한다. 머신러닝에서는 데이터에 맞게 학습하여 결정하는 파라미터 외에 사용자가 직접 조절해줘야하는 값이 존재한다. 이러한 값을 하이퍼파라미터라고 부른다.  퍼셉트론의 하이퍼파라미터는 학습률($\\eta$, eta), 반복횟수(n_iter) 두가지가 존재한다.\n",
    "\n",
    "학습률($\\eta$, eta)은 퍼셉트론의 학습알고리즘에서 가중와 편향을 조정하는데 사용한다. `(2-3) 퍼셉트론의 학습알고리즘`에서 퍼셉트론이 예측한 값과 실제값의 차이로 가중치와 편향을 조정하는데, 이 때 얼마큼 조정 할 것인가가를 학습률을 통해 결정한다. \n",
    "\n",
    "예를 들어 학습률($\\eta$) = 0.5라면 실제값과 예측값의 차이를 그대로 가중치를 업데이트하는데 활용하는 것이 아니라, 0.5를 곱한 값을 업데이트하게 된다.\n",
    "\n",
    "반복횟수(n_iter)는 몇번동안 반복해서 가중치를 조정을 할것인지를 결정한다.  \n",
    "예를 들어 반복횟수가 10회라면 총 10회 동안 가중치를 조정하고, 100회라면 100회동안 가중치를 조정한다. \n",
    "\n",
    "두 하이퍼파라미터에 대해서는 챕터 후반부에 실제 파이썬으로 구현한 퍼셉트론으로 예제를 해결하는 과정에서 상세하게 확인할 수 있으니, 현재는 가볍게 살펴보고 넘어가도록 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72b8d1",
   "metadata": {},
   "source": [
    "## 02. 퍼셉트론(Perceptron) 구현과 작동원리\n",
    "\n",
    "퍼셉트론에 대해서 간단하게 살펴보고 컨셉에대해 이해하였으니 본격적으로 파이썬을 통해 계단함수를 활성함수로 사용하는 퍼셉트론을 생성해보고 AND / OR / NAND 문제를 해결해보며 퍼셉트론의 작동원리에 대해서 자세하게 살펴보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59120431",
   "metadata": {},
   "source": [
    "### (2-1) AND, OR, NAND 문제\n",
    "\n",
    "AND, OR ,NAND 문제는 $x_{1}, x_{2}$  두 개의 입력값에 따라 하나의 출력 $y$를 어떻게 내보낼것인가에 대한 문제이다.  이 때, 신호($x$)는 1과 0으로 표현하고,출력 $y$도 0과 1로 표현한다. 우리가 풀어볼 AND, OR, NAND문제의 입력과 출력관계를 표현하면 그림[01-05]처럼 표현할 수 있다.\n",
    "\n",
    "이번 문제에서 우리의 목표는 2차원 좌표평면상에 $(x_{1},x_{2})=\\{(0,0),(0,1),(1,0),(1,1)\\}$  4개의 점이 존재할 때 각 논리게이트(AND,OR,NAND)에 맞는 결과를 반환하는 퍼셉트론을 파이썬으로 개발하는 것이다. 예를들어 AND게이트에 맞는 퍼셉트론은  $\\{(0,0),(0,1),(1,0)\\}$ 3개의 점은 0으로, $(1,1)$ 1개의 점은 1로 출력하는 알고리즘을 구현하고 학습시키는 것이다.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/int29/PDLP/blob/main/ch01_classical_neural_network/img/01_07.png?raw=true\" width=\"500\">\n",
    "    <img src=\"https://github.com/int29/PDLP/blob/main/ch01_classical_neural_network/img/01_08.png?raw=true\" width=\"500\">\n",
    "    <img src=\"https://github.com/int29/PDLP/blob/main/ch01_classical_neural_network/img/01_09.png?raw=true\" width=\"500\">\n",
    "    <span>[그림01-05]</span>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871dd80",
   "metadata": {},
   "source": [
    "아래코드는 앞서 설명한 퍼셉트론의 학습알고리즘을 그대로 구현한 파이썬 퍼셉트론 코드이다. 천천히 코드를 살펴보도록하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff195d89",
   "metadata": {},
   "source": [
    "### (2-2) 퍼셉트론 알고리즘 파이썬 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead336b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat-gpt를 통해 코드 초안 제공받고 수정함.(90%이상 GPT코드 사용)\n",
    "\n",
    "import numpy as np\n",
    "class Perceptron:\n",
    "    def __init__(self, eta=0.01, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        self.w_ = np.random.normal(loc=0.0, scale=0.01,size=X.shape[1])\n",
    "        self.b_ = np.float_(0.)\n",
    "        self.errors_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_ += update * xi\n",
    "                self.b_ += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a43d9a",
   "metadata": {},
   "source": [
    "### (2-2) 구현한 코드설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19f686",
   "metadata": {},
   "source": [
    "`__init__` 함수는 클래스가 선언되면 자동으로 선언되는 함수이다. 이 함수에서는 학습률($\\eta$) eta, 반복횟수 n_iter 를 선언한다. 기본값으로 학습률은 0.01, 반복횟수는 50회로 설정 했다.\n",
    "\n",
    "```\n",
    "def __init__(self, eta=0.01, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "`fit` 함수는 훈련데이터에 맞게 가중치와 편향을 조정하는 단계이다. \n",
    "\n",
    "```\n",
    "def fit(self, X, y):\n",
    "        self.w_ = np.random.normal(loc=0.0, scale=0.01,size=X.shape[1], rand) #(1)\n",
    "        self.b_ = np.float_(0.) #(2) \n",
    "        self.errors_ = [] #(3)\n",
    "        \n",
    "        for _ in range(self.n_iter): #(4)\n",
    "            errors = 0 #(5)\n",
    "            for xi, target in zip(X, y): #(6)\n",
    "                update = self.eta * (target - self.predict(xi)) #(7)\n",
    "                self.w_ += update * xi #(8)\n",
    "                self.b_ += update #(9)\n",
    "                errors += int(update != 0.0) #(10)\n",
    "            self.errors_.append(errors) #(11)\n",
    "        return self\n",
    "```\n",
    "\n",
    "(1) 훈련데이터에 내적가능한 형태로 랜덤 가중치 벡터를 생성한다. <br>\n",
    "(3) 랜덤가중치 상수를 생성한다.<br>\n",
    "(4) error를 저장할 버퍼 리스트를 생성한다.<br>\n",
    "(5) n_iter 만큼 반복하는 for문을 생성<br>\n",
    "(6) errors를 0으로 초기화<br>\n",
    "(7) 훈련데이터와 훈련데이터에 맞는 실제값을 zip으로 묶어서 xi, target이라는 변수에 할당함.<br>\n",
    "(8) predict함수를 fit함수내에 활용하여 예측값을 계산하고, 실제값 target과 비교 및 학습률과 곱하여 update값 생성<br>\n",
    "(9) update와 훈련데이터만큼, 가중치를 업데이트<br>\n",
    "(10) update만큼, 편향을 업데이트<br>\n",
    "(11) update가 0이아니라면 (실제값과 예측값의 차이가 발생하면) error에 1을 추가함 (Ture를 숫자로 바꾸면1로 변환됨, False는 0으로 변환됨)<br>\n",
    "\n",
    "`net_input` 함수는 훈련데이터 $x_{i}$ 에 $w$를 곱하고 $b$를 더한 값을 합한 값을 산출한다. `net_input`은 `predict`함수에 활용된다. `np.dot`은 두 벡터를 내적하는 함수이다.\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "`predict` 함수는 `net_input`에서 계산된값을 받아 0혹은 1로 반환하는 활성화 함수 역할이다.  `predict`함수는 `fit`함수에서 예측값을 계산할때도 활용된다.\n",
    " \n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb912e3",
   "metadata": {},
   "source": [
    "### (2-3) AND 문제 해결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5dfdf7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측결과는 [0 0 0 1] 입니다.\n",
      "퍼셉트론이 학습한 가중치는 : [0.29369109 0.10100702] 입니다.\n",
      "퍼셉트론이 학습한 편향은 : [-0.3] 입니다.\n"
     ]
    }
   ],
   "source": [
    "# AND 문제 데이터 정의\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# 퍼셉트론 정의 및 학습, 하이퍼파라미터는 0.05와 100회반복으로 설정\n",
    "pct = Perceptron(eta=0.1, n_iter=100)\n",
    "pct.fit(X,y)\n",
    "\n",
    "# 예측\n",
    "print(f'예측결과는 {pct.predict(X)} 입니다.')\n",
    "\n",
    "# 가중치 벡터 확인\n",
    "print(f'퍼셉트론이 학습한 가중치는 : {pct.w_} 입니다.')\n",
    "\n",
    "# 편향 벡터 확인\n",
    "print(f'퍼셉트론이 학습한 편향은 : {pct.b_} 입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd8d26",
   "metadata": {},
   "source": [
    "퍼셉트론이 가중치와 편향을 학습하여 `[0, 0, 0, 1]` 를 예측해 AND 문제를 해결한것을 확인할 수 있다.  아래와 같이 반복횟수를 점진적으로 늘려갈경우 퍼셉트론의 모델이 가중치 및 편향과 같은 파라미터를 어떻게 학습하는지 명확하게 확인할 수 있다.\n",
    "반복횟수가 0회로 설정됐을 때는 임의의 수를 가중치와 편향으로 할당했고, 결과도 `[1,0,1,0]`으로 오답을 내고있는것을 알 수 있다. \n",
    "반면 반복횟수가 1회까지로 늘어났을 때는, 1회 까지 반복해 가중치가 조정된것을 확인할 수 있다. 그렇지만 아직도 예측결과는 `[1 1 1 1]`로 AND게이트 문제를 해결하지 못하는 것을 확인 할 수 있다. 이후 반복횟수가 4회이상 부터는 경우 `[0 0 0 1]`로 AND문제를 해결한것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7fbb6a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복횟수 : 0 회 일 때, 가중치(w)는 [0.0001479 0.0036917] 이고, 편향(b)는 0.0, 예측결과는 [1 1 1 1] 입니다.\n",
      "반복횟수 : 1 회 일 때, 가중치(w)는 [0.12094613 0.10358641] 이고, 편향(b)는 [0.], 예측결과는 [1 1 1 1] 입니다.\n",
      "반복횟수 : 2 회 일 때, 가중치(w)는 [0.20757388 0.10820452] 이고, 편향(b)는 [-0.1], 예측결과는 [0 1 1 1] 입니다.\n",
      "반복횟수 : 3 회 일 때, 가중치(w)는 [0.21394076 0.18008516] 이고, 편향(b)는 [-0.1], 예측결과는 [0 1 1 1] 입니다.\n",
      "반복횟수 : 4 회 일 때, 가중치(w)는 [0.10915172 0.09620288] 이고, 편향(b)는 [-0.2], 예측결과는 [0 0 0 1] 입니다.\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,5):\n",
    "    pct = Perceptron(eta=0.1, n_iter=n)\n",
    "    pct.fit(X,y)\n",
    "    print(f'반복횟수 : {n} 회 일 때, 가중치(w)는 {pct.w_} 이고, 편향(b)는 {pct.b_}, 예측결과는 {pct.predict(X)} 입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27850922",
   "metadata": {},
   "source": [
    "### (2-4) 퍼셉트론의 하이퍼 파라미터와 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cfdc0",
   "metadata": {},
   "source": [
    "하이퍼 파라미터란 머신러닝이나 딥러닝과 같은 학습알고리즘에 영향을 주지만, 사용자가 직접 정의해줘야하는 요소를 말한다. 퍼셉트론에서는 하이퍼파라미터는 학습률($\\eta$), 반복횟수(epoch 혹은 iteration) 두가지가 존재한다.(만약 새로운 퍼셉트론 기반 알고리즘이 개발된다면 하이퍼파라미터는 달라질 수 있다.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da772738",
   "metadata": {},
   "source": [
    "#### 반복횟수(epoch)\n",
    "\n",
    "반복횟수(epoch)는 말 그대로 학습알고리즘에서 가중치와 편향을 몇 회만큼 조정을 시도 할것인지를 의미한다. 너무 큰 값을 불필요하게 반복할 경우 연산량이 늘어나 비효율적일수 있고, 너무 적은 수를 반복할 경우 모델이 충분한 성능을 내기전에 학습이 종료될 수 있다. 아래도 반복횟수를 3회 이하로 둘 경우 모델이 가중치와 편향을 올바르게 학습하기 전에 학습이 종료돼 원하는 성능을 내지 못하는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "142cf27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복횟수 : 0 회 일 때, 가중치(w)는 [-0.0054596  -0.01458735] 이고, 편향(b)는 0.0, 예측결과는 [1 0 0 0] 입니다.\n",
      "반복횟수 : 1 회 일 때, 가중치(w)는 [0.09856939 0.1055812 ] 이고, 편향(b)는 [0.], 예측결과는 [1 1 1 1] 입니다.\n",
      "반복횟수 : 2 회 일 때, 가중치(w)는 [0.1004424  0.19091095] 이고, 편향(b)는 [-0.1], 예측결과는 [0 1 1 1] 입니다.\n",
      "반복횟수 : 3 회 일 때, 가중치(w)는 [0.20468899 0.10864422] 이고, 편향(b)는 [-0.2], 예측결과는 [0 0 1 1] 입니다.\n",
      "반복횟수 : 4 회 일 때, 가중치(w)는 [0.19588546 0.00458787] 이고, 편향(b)는 [-0.2], 예측결과는 [0 0 0 1] 입니다.\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,5):\n",
    "    pct = Perceptron(eta=0.1, n_iter=n)\n",
    "    pct.fit(X,y)\n",
    "    print(f'반복횟수 : {n} 회 일 때, 가중치(w)는 {pct.w_} 이고, 편향(b)는 {pct.b_}, 예측결과는 {pct.predict(X)} 입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f245a",
   "metadata": {},
   "source": [
    "반면 이번케이스의 경우 3회 이상부터는 가중치와 편향의 값이 다소 변경되기는 하나, 결과를 잘 맞추기 때문에 4회이상 실행할 필요가 없었던 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "61a41a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복횟수 : 0 회 일 때, 가중치(w)는 [-0.00893232 -0.00337187] 이고, 편향(b)는 0.0, 예측결과는 [1 0 0 0] 입니다.\n",
      "반복횟수 : 1 회 일 때, 가중치(w)는 [0.10613571 0.11844944] 이고, 편향(b)는 [0.], 예측결과는 [1 1 1 1] 입니다.\n",
      "반복횟수 : 2 회 일 때, 가중치(w)는 [0.19674883 0.11562355] 이고, 편향(b)는 [-0.1], 예측결과는 [0 1 1 1] 입니다.\n",
      "반복횟수 : 3 회 일 때, 가중치(w)는 [0.20201973 0.18749942] 이고, 편향(b)는 [-0.1], 예측결과는 [0 1 1 1] 입니다.\n",
      "반복횟수 : 4 회 일 때, 가중치(w)는 [0.10317561 0.0972454 ] 이고, 편향(b)는 [-0.2], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 5 회 일 때, 가중치(w)는 [0.19400484 0.01507469] 이고, 편향(b)는 [-0.2], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 6 회 일 때, 가중치(w)는 [0.08211511 0.09745242] 이고, 편향(b)는 [-0.1], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 7 회 일 때, 가중치(w)는 [0.20002665 0.1925789 ] 이고, 편향(b)는 [-0.3], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 8 회 일 때, 가중치(w)는 [0.0993763  0.08877608] 이고, 편향(b)는 [-0.1], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 9 회 일 때, 가중치(w)는 [0.20671114 0.19214717] 이고, 편향(b)는 [-0.3], 예측결과는 [0 0 0 1] 입니다.\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,10):\n",
    "    pct = Perceptron(eta=0.1, n_iter=n)\n",
    "    pct.fit(X,y)\n",
    "    print(f'반복횟수 : {n} 회 일 때, 가중치(w)는 {pct.w_} 이고, 편향(b)는 {pct.b_}, 예측결과는 {pct.predict(X)} 입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2454a6b",
   "metadata": {},
   "source": [
    "#### 학습률($\\eta$)\n",
    "\n",
    "학습률($\\eta$)은 가중치와 편향을 조정할 때, 얼마만큼 점진적으로 가중치와 편향같은 파라미터를 업데이트힐 것인지를 조정하는 하이퍼파라미터 이다. 챕터 초반에 `(1-3) 퍼셉트론의 학습알고리즘`에서의 예시처럼 예측값과 실제값의 차이를 전부 반영하지 않고, 학습률만큼 점진적으로 업데이트하는것을 확인할 수 있다. \n",
    "\n",
    "학습률이 너무 클 경우 모델이 학습을 진행하다가 최적 파라미터로 수렴하지 못하고 발산하거나, 너무 적을 경우 학습 비용과 시간이 지나치게 커질 수 있어 적당한 값으로 조정해야 한다. 아래와 같이 학습률을 낮출경우 동일한 AND문제를 해결하더라도 더 많은 반복횟수가 필요한것을 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1a699ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복횟수 : 0 회 일 때, 가중치(w)는 [-0.00119454 -0.00070349] 이고, 편향(b)는 0.0, 예측결과는 [1 0 0 0] 입니다.\n",
      "반복횟수 : 1 회 일 때, 가중치(w)는 [0.00462449 0.02106756] 이고, 편향(b)는 [-0.0015], 예측결과는 [0 1 1 1] 입니다.\n",
      "반복횟수 : 2 회 일 때, 가중치(w)는 [-0.00714691  0.00684783] 이고, 편향(b)는 [-0.0005], 예측결과는 [0 1 0 0] 입니다.\n",
      "반복횟수 : 3 회 일 때, 가중치(w)는 [0.00786628 0.01427413] 이고, 편향(b)는 [-0.0035], 예측결과는 [0 1 1 1] 입니다.\n",
      "반복횟수 : 4 회 일 때, 가중치(w)는 [-5.94497347e-05 -7.63884908e-03] 이고, 편향(b)는 [0.], 예측결과는 [1 0 0 0] 입니다.\n",
      "반복횟수 : 5 회 일 때, 가중치(w)는 [ 0.0018808 -0.0044293] 이고, 편향(b)는 [-0.0005], 예측결과는 [0 0 1 0] 입니다.\n",
      "반복횟수 : 6 회 일 때, 가중치(w)는 [ 0.00353413 -0.00380697] 이고, 편향(b)는 [-0.0005], 예측결과는 [0 0 1 0] 입니다.\n",
      "반복횟수 : 7 회 일 때, 가중치(w)는 [ 0.00571505 -0.00591203] 이고, 편향(b)는 [-0.0005], 예측결과는 [0 0 1 0] 입니다.\n",
      "반복횟수 : 8 회 일 때, 가중치(w)는 [-0.0113674  -0.00152733] 이고, 편향(b)는 [0.], 예측결과는 [1 0 0 0] 입니다.\n",
      "반복횟수 : 9 회 일 때, 가중치(w)는 [0.00371346 0.00639778] 이고, 편향(b)는 [-0.007], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 10 회 일 때, 가중치(w)는 [0.00117297 0.00064864] 이고, 편향(b)는 [-0.0015], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 11 회 일 때, 가중치(w)는 [ 0.00328328 -0.00593823] 이고, 편향(b)는 [-0.0005], 예측결과는 [0 0 1 0] 입니다.\n",
      "반복횟수 : 12 회 일 때, 가중치(w)는 [-0.00590872  0.00962845] 이고, 편향(b)는 [-0.003], 예측결과는 [0 1 0 1] 입니다.\n",
      "반복횟수 : 13 회 일 때, 가중치(w)는 [ 0.00012446 -0.00712725] 이고, 편향(b)는 [0.], 예측결과는 [1 0 1 0] 입니다.\n",
      "반복횟수 : 14 회 일 때, 가중치(w)는 [0.0095371  0.02073837] 이고, 편향(b)는 [-0.0125], 예측결과는 [0 1 0 1] 입니다.\n",
      "반복횟수 : 15 회 일 때, 가중치(w)는 [0.00045417 0.00167598] 이고, 편향(b)는 [-0.002], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 16 회 일 때, 가중치(w)는 [0.00406522 0.00118386] 이고, 편향(b)는 [-0.005], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 17 회 일 때, 가중치(w)는 [0.00145453 0.00310258] 이고, 편향(b)는 [-0.004], 예측결과는 [0 0 0 1] 입니다.\n",
      "반복횟수 : 18 회 일 때, 가중치(w)는 [ 0.00815294 -0.00412937] 이고, 편향(b)는 [-0.0035], 예측결과는 [0 0 1 1] 입니다.\n",
      "반복횟수 : 19 회 일 때, 가중치(w)는 [0.00047307 0.0024308 ] 이고, 편향(b)는 [-0.0025], 예측결과는 [0 0 0 1] 입니다.\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,20):\n",
    "    pct = Perceptron(eta=0.0005, n_iter=n)\n",
    "    pct.fit(X,y)\n",
    "    print(f'반복횟수 : {n} 회 일 때, 가중치(w)는 {pct.w_} 이고, 편향(b)는 {pct.b_}, 예측결과는 {pct.predict(X)} 입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bcd08",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 튜닝과 최적화\n",
    "\n",
    "하이퍼 파라미터에 따라서 모델의 성능이 달라지기 때문에 하이퍼파라미터를 튜닝하는것은 중요한 작업이다. 마치 방정식처럼 풀 수 있는 문제라고 생각되어지나, 실제로는 공식이나 방정식처럼 계산하는 방법은 없고, 여러 하이퍼파라미터 조합을 해보면서 가장 정확도가 높거나 유사한 정확도에 성능을 적게 요하는 하이퍼파라미터로 튜닝을 진행한다.\n",
    "하이퍼 파라미터 튜닝의 경우 ANN(신경망)이후 부터 더 많이 다루기 때문에, 지금은 하이퍼파라미터가 어떤 것이고 어떤 영향을 미치는지 이해만 하고 넘어가도록 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aaee3c",
   "metadata": {},
   "source": [
    "### (2-5) OR/NAND 문제 해결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5964f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측결과는 [0 1 1 1] 입니다.\n",
      "퍼셉트론이 학습한 가중치는 : [0.18732171 0.19748906] 입니다.\n",
      "퍼셉트론이 학습한 편향은 : [-0.1] 입니다.\n"
     ]
    }
   ],
   "source": [
    "# OR 문제 데이터 정의\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# 퍼셉트론 정의 및 학습, 하이퍼파라미터는 0.05와 100회반복으로 설정\n",
    "pct = Perceptron(eta=0.1, n_iter=100)\n",
    "pct.fit(X,y)\n",
    "\n",
    "# 예측\n",
    "print(f'예측결과는 {pct.predict(X)} 입니다.')\n",
    "\n",
    "# 가중치 벡터 확인\n",
    "print(f'퍼셉트론이 학습한 가중치는 : {pct.w_} 입니다.')\n",
    "\n",
    "# 편향 벡터 확인\n",
    "print(f'퍼셉트론이 학습한 편향은 : {pct.b_} 입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b059d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측결과는 [1 1 1 0] 입니다.\n",
      "퍼셉트론이 학습한 가중치는 : [-0.18328529 -0.09212299] 입니다.\n",
      "퍼셉트론이 학습한 편향은 : [0.2] 입니다.\n"
     ]
    }
   ],
   "source": [
    "# NAND 문제 데이터 정의\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[1], [1], [1], [0]])\n",
    "\n",
    "# 퍼셉트론 정의 및 학습, 하이퍼파라미터는 0.05와 100회반복으로 설정\n",
    "pct = Perceptron(eta=0.1, n_iter=100)\n",
    "pct.fit(X,y)\n",
    "\n",
    "# 예측\n",
    "print(f'예측결과는 {pct.predict(X)} 입니다.')\n",
    "\n",
    "# 가중치 벡터 확인\n",
    "print(f'퍼셉트론이 학습한 가중치는 : {pct.w_} 입니다.')\n",
    "\n",
    "# 편향 벡터 확인\n",
    "print(f'퍼셉트론이 학습한 편향은 : {pct.b_} 입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1f4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
